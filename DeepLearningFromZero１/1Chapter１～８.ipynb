{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7f5b176",
   "metadata": {},
   "source": [
    "# Chapter１\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48761f",
   "metadata": {},
   "source": [
    "## 1.1 Numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def inspect_tensor(name, x):\n",
    "    \"\"\"\n",
    "    テンソル(配列)の情報を構造的に表示するデバッグ用関数\n",
    "    The Code Smith Recommended\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- [Inspect: {name}] ---\")\n",
    "    print(f\"Shape (形状) : {x.shape}\")\n",
    "    print(f\"Rank  (次元数): {x.ndim}\")\n",
    "    print(f\"Dtype (データ型): {x.dtype}\")\n",
    "    print(f\"Data:\\n{x}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. データの定義 (Data Definition)\n",
    "# ==========================================\n",
    "# Deep Learningでは、これを「テンソル」と呼ぶ。\n",
    "# 今回は 2x2 の行列 (Matrix) である。\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "inspect_tensor(\"A\", A)\n",
    "inspect_tensor(\"B\", B)\n",
    "\n",
    "# ==========================================\n",
    "# 2. 要素ごとの演算 (Element-wise Operations)\n",
    "# ==========================================\n",
    "# 注意: Deep Learningの数式で単に \"+\" や \"⊙\" (Hadamard product) と書かれるもの。\n",
    "# ルール: AとBの shape が完全に一致している必要がある。\n",
    "# (または後述のブロードキャストが可能であること)\n",
    "\n",
    "print(\"\\n=== 1. Element-wise Operations (要素ごと) ===\")\n",
    "\n",
    "# 足し算: 同じ位置の要素同士を足す\n",
    "# [[1+5, 2+6],\n",
    "#  [3+7, 4+8]]\n",
    "add_result = A + B\n",
    "print(f\"A + B (Addition):\\n{add_result}\")\n",
    "\n",
    "# ★重要★ 掛け算 ( ' * ' 演算子)\n",
    "# 数学的な「行列の掛け算」ではない！\n",
    "# 単なる「同じ場所にある数字同士の掛け算」である。\n",
    "# [[1*5, 2*6],\n",
    "#  [3*7, 4*8]]\n",
    "element_wise_mult = A * B\n",
    "print(f\"A * B (Element-wise Multiplication):\\n{element_wise_mult}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. 行列積 (Dot Product / Matrix Multiplication)\n",
    "# ==========================================\n",
    "# ★最重要★ Deep Learningの「層（レイヤ）」の計算はこれだ。\n",
    "# 数式: Y = X・W\n",
    "# ルール: 左側の行列の「列数」と、右側の行列の「行数」が一致しなければならない。\n",
    "# (N, M) dot (M, K) -> (N, K)\n",
    "\n",
    "print(\"\\n=== 2. Dot Product (行列積) ===\")\n",
    "\n",
    "# Python 3.5以上では '@' 演算子が推奨される (np.dot(A, B)と同じ)\n",
    "# 計算手順の可視化:\n",
    "# [[1*5 + 2*7,  1*6 + 2*8],\n",
    "#  [3*5 + 4*7,  3*6 + 4*8]]\n",
    "#\n",
    "# [[5+14,  6+16],\n",
    "#  [15+28, 18+32]]\n",
    "\n",
    "dot_result = A @ B  # または A.dot(B)\n",
    "print(f\"A @ B (Dot Product):\\n{dot_result}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. ブロードキャスト (Broadcasting)\n",
    "# ==========================================\n",
    "# NumPyの真骨頂。形状が合わない場合、自動で拡張して計算する。\n",
    "# バイアス項の加算などで多用する。\n",
    "\n",
    "print(\"\\n=== 3. Broadcasting (ブロードキャスト) ===\")\n",
    "\n",
    "C = np.array([10, 20])  # Shape: (2,)\n",
    "inspect_tensor(\"C (Scalar-like vector)\", C)\n",
    "\n",
    "# Aは (2, 2)、Cは (2,)\n",
    "# NumPyはCを勝手に [[10, 20], [10, 20]] に拡張(コピー)して計算してくれる。\n",
    "broadcast_result = A * C\n",
    "print(f\"A * C (Broadcasting):\\n{broadcast_result}\")\n",
    "# 解説:\n",
    "# [1, 2] * [10, 20] -> [10, 40]\n",
    "# [3, 4] * [10, 20] -> [30, 80]\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. よく使う便利機能 (Common Utils)\n",
    "# ==========================================\n",
    "print(\"\\n=== 4. Useful Utils ===\")\n",
    "\n",
    "print(\"転置 (Transpose): 行と列を入れ替える\")\n",
    "print(\"例: 逆伝播(Backpropagation)の実装で死ぬほど使う。\")\n",
    "print(f\"A.T (Transpose):\\n{A.T}\")\n",
    "\n",
    "print(\"\\nFlatten: 一列に並べる\")\n",
    "print(\"例: 画像データ(2次元)をニューラルネットワークの入力(1次元)にする時に使う。\")\n",
    "print(f\"A.flatten(): {A.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e713de",
   "metadata": {},
   "source": [
    "## 1.2 Matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7178ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 0. データ生成 (Data Generation)\n",
    "# ==========================================\n",
    "# グラフを描くには、まず「x軸の点」が必要だ。\n",
    "# np.arange(start, end, step): step刻みで生成\n",
    "# np.linspace(start, end, num): startからendまでをnum等分 (滑らかな曲線用)\n",
    "\n",
    "x = np.arange(0, 6, 0.1)  # 0から6まで0.1刻み [0, 0.1, ... 5.9]\n",
    "y1 = np.sin(x)  # 正弦波\n",
    "y2 = np.cos(x)  # 余弦波\n",
    "y3 = np.tan(x)\n",
    "\n",
    "print(\"Data Prepared.\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. 基本的なグラフ描画 (Basic Plotting)\n",
    "# ==========================================\n",
    "# Deep Learningでは「活性化関数」や「損失関数の推移」を見るのに使う。\n",
    "\n",
    "# キャンバス(Figure)の準備。これがないと始まらない。\n",
    "plt.figure(figsize=(8, 5))  # 横8インチ, 縦5インチ\n",
    "\n",
    "# プロット実行\n",
    "# label引数は、後の legend() で凡例を表示するために必須。\n",
    "plt.plot(x, y1, label=\"sin(x)\")\n",
    "plt.plot(x, y2, linestyle=\"--\", label=\"cos(x)\")  # 破線にする\n",
    "# plt.plot(x, y3, linestyle=\"--\", label=\"tan(x)\")\n",
    "\n",
    "# 装飾 (これがないグラフはエンジニア失格である)\n",
    "plt.xlabel(\"x value\")  # x軸のラベル\n",
    "plt.ylabel(\"y value\")  # y軸のラベル\n",
    "plt.title(\"Activation Function (Demo)\")  # タイトル\n",
    "plt.legend()  # 凡例を表示 (左下の線とか)\n",
    "plt.grid(True)  # グリッド線 (値を見やすくする)\n",
    "\n",
    "# 表示\n",
    "print(\"Displaying Plot 1...\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. 画像の表示 (Image Visualization)\n",
    "# ==========================================\n",
    "# Deep Learning (CNN等) では、画像を「行列」として扱う。\n",
    "# その行列が「人間の目にどう見えるか」を確認する。\n",
    "\n",
    "# ダミー画像データの生成 (28x28ピクセル)\n",
    "# np.random.rand -> 0.0~1.0の乱数\n",
    "img_data = np.random.rand(28, 28)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "# imshow: 行列を画像として表示\n",
    "# cmap='gray': グレースケールで表示 (MNISTなどで必須)\n",
    "# interpolation='nearest': 補間なし (ピクセルをそのまま表示)\n",
    "plt.imshow(img_data, cmap=\"gray\", interpolation=\"nearest\")\n",
    "\n",
    "plt.title(\"Random Noise (28x28)\")\n",
    "plt.colorbar()  # 横にカラーバーを表示 (値の大小がわかる)\n",
    "plt.axis(\"off\")  # 軸目盛りを消す (画像には不要)\n",
    "\n",
    "print(\"Displaying Plot 2 (Image)...\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. 複数のグラフを並べる (Subplots)\n",
    "# ==========================================\n",
    "# 「学習データの正解率」と「損失」を同時に見たい時などに使う。\n",
    "# The Code Smith Recommended: plt.subplots() が現代的な書き方だ。\n",
    "\n",
    "# 1行2列のキャンバスを作成\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "# 1つ目のグラフ (左側: axes[0])\n",
    "axes[0].plot(x, y1, color=\"blue\")\n",
    "axes[0].set_title(\"Training Accuracy\")\n",
    "axes[0].set_xlabel(\"Epochs\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# 2つ目のグラフ (右側: axes[1])\n",
    "axes[1].plot(x, y2, color=\"red\")\n",
    "axes[1].set_title(\"Training Loss\")\n",
    "axes[1].set_xlabel(\"Epochs\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()  # レイアウトの崩れを自動調整\n",
    "print(\"Displaying Plot 3 (Subplots)...\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c94d48",
   "metadata": {},
   "source": [
    "# Chapter２\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731fb8c",
   "metadata": {},
   "source": [
    "## 2.1 Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95582d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 1. パーセプトロンの基本定義 (The Core)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def perceptron_core(x1, x2, w1, w2, b):\n",
    "    \"\"\"\n",
    "    パーセプトロンの核となる計算\n",
    "    Input: x1, x2 (入力), w1, w2 (重み), b (バイアス)\n",
    "    Output: 0 or 1\n",
    "    \"\"\"\n",
    "    # 入力信号と重みの積の総和 + バイアス\n",
    "    # ここはまだ行列積(np.dot)を使わず、スカラーで明示的に書く\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([w1, w2])\n",
    "\n",
    "    # tmp = w1*x1 + w2*x2 + b\n",
    "    tmp = np.sum(w * x) + b\n",
    "\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. 論理ゲートの実装 (Logic Gates)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def AND(x1, x2):\n",
    "    # 重みとバイアスは人間が決める (学習ではない)\n",
    "    w1, w2, b = 0.5, 0.5, -0.7\n",
    "    return perceptron_core(x1, x2, w1, w2, b)\n",
    "\n",
    "\n",
    "def NAND(x1, x2):\n",
    "    # ANDのパラメータの符号を逆転させればよい\n",
    "    w1, w2, b = -0.5, -0.5, 0.7\n",
    "    return perceptron_core(x1, x2, w1, w2, b)\n",
    "\n",
    "\n",
    "def OR(x1, x2):\n",
    "    # バイアスを調整して、どちらか一方が1なら発火するようにする\n",
    "    w1, w2, b = 0.5, 0.5, -0.2\n",
    "    return perceptron_core(x1, x2, w1, w2, b)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. 動作確認 (Testing)\n",
    "# ==========================================\n",
    "print(\"--- Logic Gate Tests ---\")\n",
    "inputs = [(0, 0), (1, 0), (0, 1), (1, 1)]\n",
    "\n",
    "for x1, x2 in inputs:\n",
    "    y = AND(x1, x2)\n",
    "    print(f\"AND({x1}, {x2}) -> {y}\")\n",
    "\n",
    "print(\"-\" * 20)\n",
    "for x1, x2 in inputs:\n",
    "    y = OR(x1, x2)\n",
    "    print(f\"OR({x1}, {x2})  -> {y}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. 決定境界の可視化 (Visualization)\n",
    "# ==========================================\n",
    "# ここが重要だ。\n",
    "# w1*x + w2*y + b = 0 という直線をプロットする。\n",
    "# 式変形すると: y = -(w1/w2)x - (b/w2)\n",
    "\n",
    "\n",
    "def plot_decision_boundary(func, w1, w2, b, title):\n",
    "    \"\"\"\n",
    "    論理ゲートの入力点(0,0)...(1,1)と、それを分ける直線をプロットする\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.title(f\"Decision Boundary: {title}\")\n",
    "\n",
    "    # 1. 入力点のプロット\n",
    "    # 出力が 1 なら丸(o)、0 なら バツ(x) で描く\n",
    "    for x1, x2 in inputs:\n",
    "        res = func(x1, x2)\n",
    "        marker = \"o\" if res == 1 else \"x\"\n",
    "        color = \"blue\" if res == 1 else \"red\"\n",
    "        plt.scatter(x1, x2, s=200, marker=marker, c=color)\n",
    "\n",
    "    # 2. 決定境界(直線)の描画\n",
    "    # x軸の範囲\n",
    "    x_range = np.arange(-0.5, 1.5, 0.1)\n",
    "\n",
    "    # y = -(w1*x + b) / w2\n",
    "    # w2が0の場合は除算エラーになるので注意（今回は固定値なので無視）\n",
    "    y_range = -(w1 * x_range + b) / w2\n",
    "\n",
    "    plt.plot(x_range, y_range, linestyle=\"--\", color=\"green\", label=\"Boundary Line\")\n",
    "\n",
    "    # 装飾\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "\n",
    "    # 塗りつぶし (w1*x + w2*y + b > 0 の領域)\n",
    "    # 視覚的にわかりやすくするため、背景を薄く塗る\n",
    "    x_fill = np.linspace(-0.5, 1.5, 100)\n",
    "    y_fill = np.linspace(-0.5, 1.5, 100)\n",
    "    X, Y = np.meshgrid(x_fill, y_fill)\n",
    "    Z = w1 * X + w2 * Y + b\n",
    "    plt.contourf(\n",
    "        X, Y, Z, levels=[-100, 0, 100], colors=[\"#ffcccc\", \"#ccffcc\"], alpha=0.2\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nDisplaying Decision Boundaries...\")\n",
    "# ANDゲートの可視化 (w=0.5, 0.5, b=-0.7)\n",
    "plot_decision_boundary(AND, 0.5, 0.5, -0.7, \"AND Gate\")\n",
    "\n",
    "# ORゲートの可視化 (w=0.5, 0.5, b=-0.2)\n",
    "plot_decision_boundary(OR, 0.5, 0.5, -0.2, \"OR Gate\")\n",
    "\n",
    "# NANDゲートの可視化 (w=-0.5, -0.5, b=0.7)\n",
    "plot_decision_boundary(NAND, -0.5, -0.5, 0.7, \"NAND Gate\")\n",
    "# （...先ほどのコードの続き...）\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"【⚡ Deus Ex Machina's Lecture: The Blade of Linearity】\")\n",
    "print(\"=\" * 60)\n",
    "print(\"まりよ、画面に描かれた「緑色の直線」を凝視せよ。\")\n",
    "print(\"これがパーセプトロンの正体、すなわち「線形分離（Linear Separation）」だ。\")\n",
    "print(\"\\n数式 w1*x1 + w2*x2 + b = 0 は、空間を一刀両断する「刀」である。\")\n",
    "print(\" - 重み (w) は、刀の「角度」を決める。\")\n",
    "print(\" - バイアス (b) は、刀を振り下ろす「位置」を決める。\")\n",
    "print(\n",
    "    \"\\nANDもORもNANDも、所詮はこの「一本の刀」で白黒を分けられる単純な問題に過ぎない。\"\n",
    ")\n",
    "print(\"だが侮るな。この単純なニューロンを億単位で束ね、\")\n",
    "print(\"非線形な「深さ」を与えた時、それは言語を操り、画像を理解する「Polaris」となる。\")\n",
    "print(\"全ての偉大な知能は、この単純な一次方程式から始まるのだ。\")\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075e5b3",
   "metadata": {},
   "source": [
    "## 2.2 Multi-Layer Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9207a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 1. パーセプトロンの基本定義 (The Core)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def perceptron_core(x1, x2, w1, w2, b):\n",
    "    \"\"\"\n",
    "    【解説】パーセプトロンの心臓部\n",
    "    数式: y = 1 if (w1*x1 + w2*x2 + b > 0) else 0\n",
    "\n",
    "    ここはまだ「行列積 (np.dot)」を使わず、構造が見えやすいように記述している。\n",
    "    Chapter 3以降は np.dot(X, W) + B に進化する。\n",
    "    \"\"\"\n",
    "    x = np.array([x1, x2])  # 入力信号\n",
    "    w = np.array([w1, w2])  # 重み（信号の重要度）\n",
    "\n",
    "    # 信号の総和を計算 (重み付き和 + バイアス)\n",
    "    # np.sum([w1*x1, w2*x2]) + b\n",
    "    tmp = np.sum(w * x) + b\n",
    "\n",
    "    # ステップ関数 (Step Function) の適用\n",
    "    # 0を境にして、1か0かを出力する（活性化関数の原型）\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. 基本論理ゲート (Logic Gates)\n",
    "# ==========================================\n",
    "# 重み(w)とバイアス(b)は、人間が「手作業」で決めている。\n",
    "# Deep Learningでは、これを「AI自身に見つけさせる」ことになる。\n",
    "\n",
    "\n",
    "def AND(x1, x2):\n",
    "    # 両方1の時だけ発火させたい\n",
    "    # 0.5 + 0.5 - 0.7 = 0.3 (>0) -> OK\n",
    "    # 0.5 + 0.0 - 0.7 = -0.2 (<=0) -> NG\n",
    "    return perceptron_core(x1, x2, 0.5, 0.5, -0.7)\n",
    "\n",
    "\n",
    "def NAND(x1, x2):\n",
    "    # ANDの逆 (Not AND)。重みとバイアスの符号を逆にすれば実現できる。\n",
    "    # 「否定」の感情を持つニューロンと言える。\n",
    "    return perceptron_core(x1, x2, -0.5, -0.5, 0.7)\n",
    "\n",
    "\n",
    "def OR(x1, x2):\n",
    "    # どちらか片方でもあれば発火させたい\n",
    "    # バイアスを浅く(-0.2)することで、興奮しやすくしている。\n",
    "    return perceptron_core(x1, x2, 0.5, 0.5, -0.2)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. XORゲートの実装 (Multi-Layer Perceptron)\n",
    "# ==========================================\n",
    "# ★ここが本書のクライマックス★\n",
    "# 単層（直線1本）では解けないXORを、層を重ねることで解く。\n",
    "\n",
    "\n",
    "def XOR(x1, x2):\n",
    "    \"\"\"\n",
    "    【解説】XORの仕組み\n",
    "    入力層 -> 第1層(NAND, OR) -> 第2層(AND) -> 出力\n",
    "\n",
    "    例: (1, 1) の場合\n",
    "    s1 = NAND(1, 1) -> 0\n",
    "    s2 = OR(1, 1)   -> 1\n",
    "    y  = AND(0, 1)  -> 0  (正解！)\n",
    "    \"\"\"\n",
    "    s1 = NAND(x1, x2)  # 第1層ニューロン1\n",
    "    s2 = OR(x1, x2)  # 第1層ニューロン2\n",
    "    y = AND(s1, s2)  # 第2層ニューロン\n",
    "    return y\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. 決定境界の可視化 (Visualization Logic)\n",
    "# ==========================================\n",
    "# ※ここからは「結果をどう見せるか」という分析技術の話だ。\n",
    "# 　AI開発において、デバッグのために可視化コードを書く能力は必須である。\n",
    "\n",
    "\n",
    "def plot_decision_boundary_mesh(func, title):\n",
    "    # 描画キャンバスの準備\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.title(f\"Decision Boundary: {title}\")\n",
    "\n",
    "    # 1. 領域データの作成 (Meshgrid)\n",
    "    # グラフの背景全体を塗りつぶすために、細かいグリッド点を作る。\n",
    "    # -0.5 から 1.5 まで 0.01 刻みの座標点を作成\n",
    "    # xx: x座標の行列, yy: y座標の行列\n",
    "    xx, yy = np.meshgrid(np.arange(-0.5, 1.5, 0.01), np.arange(-0.5, 1.5, 0.01))\n",
    "\n",
    "    # 2. 背景の各点についてニューラルネットの出力を計算\n",
    "    # グリッド上のすべての点 (x, y) をモデルに入力し、0か1かを判定させる。\n",
    "    Z = []\n",
    "    # ravel()で1列に並べ直してループ処理 (vectorize非対応関数のため)\n",
    "    for x, y in zip(xx.ravel(), yy.ravel()):\n",
    "        Z.append(func(x, y))\n",
    "\n",
    "    # 計算結果を元のグリッド形状に戻す\n",
    "    Z = np.array(Z).reshape(xx.shape)\n",
    "\n",
    "    # 3. 等高線 (Contour) で塗りつぶし\n",
    "    # Zの値に基づいて色を塗る。これが「AIが見ている世界の境界線」である。\n",
    "    # levels=[-0.1, 0.9, 1.1] -> 0付近と1付近で色を分ける設定\n",
    "    plt.contourf(\n",
    "        xx,\n",
    "        yy,\n",
    "        Z,\n",
    "        levels=[-0.1, 0.9, 1.1],\n",
    "        colors=[\"#ffcccc\", \"#ccffcc\"],  # 赤っぽい色(0), 緑っぽい色(1)\n",
    "        alpha=0.3,  # 透明度\n",
    "    )\n",
    "\n",
    "    # 4. 正解データのプロット (入力点4つ)\n",
    "    inputs = [(0, 0), (1, 0), (0, 1), (1, 1)]\n",
    "    for x1, x2 in inputs:\n",
    "        res = func(x1, x2)\n",
    "        # 正解(1)なら緑の丸、不正解(0)なら赤のバツ\n",
    "        marker = \"o\" if res == 1 else \"x\"\n",
    "        color = \"green\" if res == 1 else \"red\"\n",
    "\n",
    "        plt.scatter(\n",
    "            x1,\n",
    "            x2,\n",
    "            s=200,  # サイズ\n",
    "            marker=marker,  # 形\n",
    "            c=color,  # 色\n",
    "            edgecolors=\"black\",  # 縁取り\n",
    "            linewidths=1.5,\n",
    "            zorder=10,  # 最前面に表示\n",
    "        )\n",
    "\n",
    "    # グラフの体裁を整える\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.grid(linestyle=\"--\", alpha=0.7)\n",
    "    plt.xlabel(\"x1 (Input 1)\")\n",
    "    plt.ylabel(\"x2 (Input 2)\")\n",
    "\n",
    "    print(f\"Displaying {title}...\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 実行 (Execution)\n",
    "# ==========================================\n",
    "print(\"--- XOR Gate Logic Check ---\")\n",
    "for x1, x2 in [(0, 0), (1, 0), (0, 1), (1, 1)]:\n",
    "    print(f\"Input({x1}, {x2}) -> Output: {XOR(x1, x2)}\")\n",
    "\n",
    "# グラフ描画\n",
    "plot_decision_boundary_mesh(XOR, \"XOR Gate (Multi-Layer)\")\n",
    "\n",
    "# （...先ほどのコードの続きとして、以下のprint文を追加、あるいは脳内で補完せよ...）\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"【⚡ Deus Ex Machina's Lecture: The Power of 'Depth'】\")\n",
    "print(\"=\" * 60)\n",
    "print(\"まりよ、画面上の決定境界（Decision Boundary）をよく見よ。\")\n",
    "print(\"単層パーセプトロンでは、世界を「直線」でしか切り取れなかった。\")\n",
    "print(\"だが、層を重ねる（Multi-Layer）ことで、汝は「曲線」や「領域」を作り出した。\")\n",
    "print(\"\\nこれは単なるパズルではない。Deep Learningの本質的意味だ。\")\n",
    "print(\"1. 第1層 (NAND, OR) は、入力 x を「新しい空間 s」に変換した。\")\n",
    "print(\"   - (0,0) -> (1,0)\")\n",
    "print(\"   - (1,0) -> (1,1)\")\n",
    "print(\"   - ...\")\n",
    "print(\"2. 第2層 (AND) は、その変換されたデータを見て、線形分離したに過ぎない。\")\n",
    "print(\n",
    "    \"\\n即ち、層を深くするとは「問題を解きやすい形にデータを変形（表現）すること」である。\"\n",
    ")\n",
    "print(\"このXORゲートこそが、Polarisのような高度なAIを作るための「最初の一歩」だ。\")\n",
    "print(\"「線形」の呪縛から解き放たれた今、我々に表現できない関数は存在しない。\")\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27f6a5",
   "metadata": {},
   "source": [
    "# Chapter３\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e7c59",
   "metadata": {},
   "source": [
    "## 3.1 Activation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e18a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 0. 準備: 定義域の設定\n",
    "# ==========================================\n",
    "# グラフを描くために、-5.0 から 5.0 までの数値を 0.1 刻みで用意する\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3.2.2 & 3.2.3 ステップ関数 (Step Function)\n",
    "# ==========================================\n",
    "# パーセプトロンで使われていた「カクカクした」関数。\n",
    "# 閾値を超えたら「1」、超えなければ「0」。\n",
    "\n",
    "\n",
    "def step_function(x):\n",
    "    \"\"\"\n",
    "    【解説】\n",
    "    x > 0 の評価結果は bool型 (True/False) になる。\n",
    "    これを .astype(np.int64) で int型 (1/0) に変換する。\n",
    "    \"\"\"\n",
    "    # print(f\"  [Step Debug] Input:\\n{x}\")\n",
    "    # print(f\"  [Step Debug] Bool:\\n{x > 0}\")\n",
    "    return np.array(x > 0, dtype=np.int64)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3.2.1 & 3.2.4 シグモイド関数 (Sigmoid Function)\n",
    "# ==========================================\n",
    "# 歴史ある活性化関数。「S字カーブ」を描く。\n",
    "# 数式: h(x) = 1 / (1 + exp(-x))\n",
    "# 重要: 出力が常に 0.0 〜 1.0 の間に収まる → 「確率」として解釈できる。\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    【解説】\n",
    "    np.exp(-x) は自然対数の底 e の -x 乗。\n",
    "    NumPyのブロードキャスト機能により、配列xの全要素に対して一気に計算される。\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3.2.7 ReLU関数 (Rectified Linear Unit)\n",
    "# ==========================================\n",
    "# 現代のDeep Learning (特に画像処理やLLM) のスタンダード。\n",
    "# x > 0 ならそのまま x を出力。 x <= 0 なら 0 を出力。\n",
    "# シグモイドより計算が高速で、学習が進みやすい（勾配消失しにくい）。\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    【解説】\n",
    "    np.maximum(0, x) は、0 と x を比較して大きい方を返す。\n",
    "    if文を使わずに書けるため高速。\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 実行と可視化 (Visualization)\n",
    "# ==========================================\n",
    "print(\"\\n--- 1. Step Function Output (Example) ---\")\n",
    "# 試しに -1.0 と 2.0 を入れてみる\n",
    "sample_in = np.array([-1.0, 2.0])\n",
    "print(f\"Input: {sample_in}\")\n",
    "print(f\"Output: {step_function(sample_in)}\")  # [0, 1] になるはず\n",
    "\n",
    "\n",
    "print(\"\\n--- 2. Sigmoid Function Output (Example) ---\")\n",
    "# 0を入れるとちょうど 0.5 になるのが特徴\n",
    "sample_in = np.array([0, -100, 100])\n",
    "print(f\"Input: {sample_in}\")\n",
    "print(f\"Output: {sigmoid(sample_in)}\")\n",
    "# 0 -> 0.5, -100 -> 0.0に近い, 100 -> 1.0に近い\n",
    "\n",
    "\n",
    "print(\"\\n--- 3. Plotting Graphs (Comparison) ---\")\n",
    "\n",
    "y_step = step_function(x)\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_relu = relu(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# ステップ関数: 破線\n",
    "plt.plot(x, y_step, linestyle=\"--\", label=\"Step Function\", color=\"black\")\n",
    "\n",
    "# シグモイド関数: 青い実線\n",
    "plt.plot(x, y_sigmoid, label=\"Sigmoid\", color=\"blue\")\n",
    "\n",
    "# ReLU関数: 赤い実線 (値が大きくなるので、y軸の範囲に注意)\n",
    "plt.plot(x, y_relu, label=\"ReLU\", color=\"red\", alpha=0.5)\n",
    "\n",
    "plt.ylim(\n",
    "    -0.1, 1.1\n",
    ")  # シグモイドとステップを見やすくするためy軸を制限 (ReLUの上の方は切れる)\n",
    "plt.title(\"Activation Functions Comparison\")\n",
    "plt.xlabel(\"x (Input)\")\n",
    "plt.ylabel(\"y (Output)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"Graphs displayed.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3.2.6 非線形関数 (Non-linearity) の重要性\n",
    "# ==========================================\n",
    "print(\"\\n【Deus Ex Machina's Lecture】\")\n",
    "print(\"なぜ「活性化関数」が必要なのか？\")\n",
    "print(\"もし h(x) = cx のような「線形関数」を使ってしまうと...\")\n",
    "print(\"3層重ねても y(x) = h(h(h(x))) = c * c * c * x = c^3 * x\")\n",
    "print(\"これでは、ただの「1層の線形変換」と同じになってしまう（隠れ層の意味がない）。\")\n",
    "print(\"だから、Deep Learningには、曲がった（非線形な）関数が絶対に不可欠なのだ。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85183e7",
   "metadata": {},
   "source": [
    "## 3.2 Forward Propagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 0. ユーティリティ関数 (Tools)\n",
    "# ==========================================\n",
    "def inspect(name, x):\n",
    "    \"\"\"変数の形状(Shape)と中身を確認するデバッグ関数\"\"\"\n",
    "    print(f\"\\n[Inspect: {name}]\")\n",
    "    print(f\"  Shape: {x.shape}\")\n",
    "    print(f\"  Value:\\n{x}\")\n",
    "\n",
    "\n",
    "# 活性化関数たち (Chapter 3.2 Recap)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def identity_function(x):\n",
    "    \"\"\"恒等関数: 入力をそのまま出力する (回帰問題などで使用)\"\"\"\n",
    "    return x\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3.3 多次元配列の計算 (Matrix Dot Product)\n",
    "# ==========================================\n",
    "# ニューラルネットワークの計算は、結局のところ「行列の積」である。\n",
    "# Y = X・W + B\n",
    "# これを理解せずに先に進むことは許されない。\n",
    "\n",
    "\n",
    "def demo_matrix_multiplication():\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\" >>> 3.3 Matrix Dot Product Demo\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 入力 X: (2,) -> これを (1, 2) の行列として扱うのがコツ\n",
    "    X = np.array([1, 2])\n",
    "\n",
    "    # 重み W: (2, 3) -> 入力が2個、次の層のニューロンが3個あることを意味する\n",
    "    W = np.array([[1, 3, 5], [2, 4, 6]])\n",
    "\n",
    "    # 計算: X(1,2) dot W(2,3) -> Y(1,3)\n",
    "    # 行列積のルール: 左の「列数」と右の「行数」が一致しなければならない (2と2で一致)\n",
    "    Y = np.dot(X, W)\n",
    "\n",
    "    print(\"Calculation: Y = X・W\")\n",
    "    inspect(\"Input X\", X)\n",
    "    inspect(\"Weight W\", W)\n",
    "    inspect(\"Result Y\", Y)\n",
    "\n",
    "    print(\"\\n【解説】\")\n",
    "    print(f\"Xの形状{X.shape} と Wの形状{W.shape} が結合し、\")\n",
    "    print(f\"Yの形状{Y.shape} が生成された。これが「層の移動」である。\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3.5 ソフトマックス関数 (Softmax)\n",
    "# ==========================================\n",
    "# 分類問題における出力層の「定番」。\n",
    "# 出力の総和が必ず「1.0」になるため、「確率」として扱える。\n",
    "\n",
    "\n",
    "def softmax(a):\n",
    "    \"\"\"\n",
    "    Over-flow対策済みのSoftmax関数\n",
    "    \"\"\"\n",
    "    # そのままだと exp(1000) などで無限大(nan)になるため、最大値を引く\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)  # オーバーフロー対策\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3.4 3層ニューラルネットワークの実装\n",
    "# ==========================================\n",
    "# ここが本番だ。\n",
    "# 入力層(2つ) -> 第1層(3つ) -> 第2層(2つ) -> 出力層(2つ)\n",
    "# というネットワークを構築し、信号を流す。\n",
    "\n",
    "\n",
    "def init_network():\n",
    "    \"\"\"\n",
    "    重みとバイアスを初期化する (今回は学習済みと仮定して固定値を設定)\n",
    "    \"\"\"\n",
    "    network = {}\n",
    "\n",
    "    # 第1層 (Input:2 -> Hidden1:3)\n",
    "    network[\"W1\"] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network[\"b1\"] = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "    # 第2層 (Hidden1:3 -> Hidden2:2)\n",
    "    network[\"W2\"] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network[\"b2\"] = np.array([0.1, 0.2])\n",
    "\n",
    "    # 出力層 (Hidden2:2 -> Output:2)\n",
    "    network[\"W3\"] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network[\"b3\"] = np.array([0.1, 0.2])\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def forward(network, x):\n",
    "    \"\"\"\n",
    "    順伝播 (Forward Propagation) の処理\n",
    "    入力 x から 出力 y を求めるプロセス\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\" >>> 3.4 Forward Propagation Start\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    W1, W2, W3 = network[\"W1\"], network[\"W2\"], network[\"W3\"]\n",
    "    b1, b2, b3 = network[\"b1\"], network[\"b2\"], network[\"b3\"]\n",
    "\n",
    "    # --- 第1層への伝達 ---\n",
    "    # a: 活性化関数を通す前の生データ (pre-activation)\n",
    "    # z: 活性化関数を通した後の信号 (post-activation)\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "\n",
    "    print(f\"Layer 1 (Sigmoid): Input{x.shape} -> Dot{a1.shape} -> Output{z1.shape}\")\n",
    "    # print(f\"  z1: {z1}\") # 詳細見たい場合はコメントアウト解除\n",
    "\n",
    "    # --- 第2層への伝達 ---\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "\n",
    "    print(f\"Layer 2 (Sigmoid): Input{z1.shape} -> Dot{a2.shape} -> Output{z2.shape}\")\n",
    "\n",
    "    # --- 出力層への伝達 ---\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "\n",
    "    # 出力層の活性化関数は、問題を解く種類によって変える\n",
    "    # 回帰問題 -> 恒等関数 (identity_function)\n",
    "    # 分類問題 -> ソフトマックス (softmax)\n",
    "    y = identity_function(a3)\n",
    "\n",
    "    print(f\"Layer 3 (Identity): Input{z2.shape} -> Dot{a3.shape} -> Output{y.shape}\")\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Main Execution\n",
    "# ==========================================\n",
    "\n",
    "# 1. 行列積のデモ\n",
    "demo_matrix_multiplication()\n",
    "\n",
    "# 2. ニューラルネットワークの推論\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])  # 入力データ\n",
    "y = forward(network, x)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(f\"Final Output (Raw): {y}\")\n",
    "\n",
    "# 3. ソフトマックス関数の適用 (3.5節)\n",
    "# 推論結果を「確率」に変換する\n",
    "probability = softmax(y)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\" >>> 3.5 Output Layer (Softmax)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Softmax Output: {probability}\")\n",
    "print(f\"Sum of Probability: {np.sum(probability)}\")  # 必ず 1.0 になる\n",
    "\n",
    "print(\"\\n【Deus Ex Machina's Analysis】\")\n",
    "print(\"見よ。入力 [1.0, 0.5] が、層をくぐるたびに行列積で変形され、\")\n",
    "print(\"最終的に [0.316..., 0.696...] という確率分布に生まれ変わった。\")\n",
    "print(\"これは「2番目のクラスである確率が約70%」とAIが判断したことを意味する。\")\n",
    "print(\"行列演算こそが、思考の正体だ。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a9586",
   "metadata": {},
   "source": [
    "## 3.3 MNIST Inference & Batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34b794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Loading MNIST Data...]\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " >>> 3.6.2 Inference (Single Data)\n",
      "==================================================\n",
      "Input Shape: (784,)\n",
      "Output Probabilities: \n",
      "[0.26837697 0.04688931 0.17861669 0.06555113 0.08469671 0.03518245\n",
      " 0.13122941 0.09240204 0.04770964 0.04934564]\n",
      "Predicted Label: 0 (Correct: 7)\n",
      "※ ランダム重みなので外れて当然である。\n",
      "\n",
      "==================================================\n",
      " >>> 3.6.3 Batch Processing (High Speed)\n",
      "==================================================\n",
      "Total Images: 10000\n",
      "Batch Size: 100\n",
      "\n",
      "Accuracy: 0.0980\n",
      "※ 約0.1 (10%) になれば正常。10個の数字を適当に選んでいるだけだからだ。\n",
      "\n",
      "============================================================\n",
      "【⚡ Deus Ex Machina's Lecture: The Architecture of Thought】\n",
      "============================================================\n",
      "まりよ、今まさに汝のPC内で「1万枚の画像」が瞬時に処理された。\n",
      "このスピードの秘密こそが「バッチ処理（Batch Processing）」と「行列演算」だ。\n",
      "\n",
      "1. データの変形 (Flatten):\n",
      "   28x28の「絵」は、784次元の「数字の列」に解体された。\n",
      "   AIにとって、画像とは「空間」ではなく「信号の羅列」に過ぎない。\n",
      "\n",
      "2. 情報の圧縮と抽象化:\n",
      "   784個の信号は、重みW1によって50個に圧縮され、W2で100個に拡張され、\n",
      "   最後にW3で「10個の確率（0〜9）」へと変換された。\n",
      "   [Image of Neural Network Architecture]\n",
      "   この「層」をくぐる過程で、情報は『ピクセルの明暗』から『数字の特徴』へと昇華される。\n",
      "\n",
      "3. 現状の課題:\n",
      "   Accuracyは約0.1 (10%)。これは重みパラメータが「ランダム」だからだ。\n",
      "   回路（脳の構造）は完成している。だが、シナプス（重み）が繋がっていない。\n",
      "\n",
      "次章『学習』にて、我々はこの重み W を、データから自動的に修正させる。\n",
      "その時、この回路はただの計算機から「知能」へと進化するだろう。\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# ==========================================\n",
    "# 1. MNISTデータのロード (Data Loading)\n",
    "# ==========================================\n",
    "# 本来は 'dataset.mnist' を使うが、ここでは汎用的な Keras を使ってロードする。\n",
    "# 重要なのは「どうロードするか」ではなく「ロード後の shape」だ。\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    print(\"\\n[Loading MNIST Data...]\")\n",
    "    try:\n",
    "        from tensorflow.keras.datasets import mnist\n",
    "\n",
    "        (x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "        print(\"Done.\")\n",
    "    except ImportError:\n",
    "        print(\"Warning: TensorFlow not found. Using Dummy Data.\")\n",
    "        # ダミーデータ (動作確認用)\n",
    "        x_test = np.random.rand(1000, 28, 28)\n",
    "        t_test = np.random.randint(0, 10, 1000)\n",
    "\n",
    "    # --- 前処理 (Pre-processing) ---\n",
    "    # 1. Flatten: 画像 (N, 28, 28) を (N, 784) に平坦化する\n",
    "    #    ニューラルネットの入力層は「一列」である必要があるため。\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "    # 2. Normalize: 0~255 の値を 0.0~1.0 に変換する\n",
    "    #    値が大きいとオーバーフローしたり学習が進みにくいため。\n",
    "    x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "    return x_test, t_test\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. ネットワークの初期化 (Initialization)\n",
    "# ==========================================\n",
    "# 本来は 'sample_weight.pkl' (学習済み重み) を読むが、\n",
    "# ここでは構造理解のために「ランダムな重み」で初期化する。\n",
    "# 当然、正解率は約10% (デタラメ) になるが、信号は流れる。\n",
    "\n",
    "\n",
    "def init_network():\n",
    "    network = {}\n",
    "    # 入力層: 784 (28x28)\n",
    "    # 隠れ層1: 50\n",
    "    # 隠れ層2: 100\n",
    "    # 出力層: 10 (0~9の数字)\n",
    "\n",
    "    weight_scale = 0.1\n",
    "    network[\"W1\"] = weight_scale * np.random.randn(784, 50)\n",
    "    network[\"b1\"] = np.zeros(50)\n",
    "\n",
    "    network[\"W2\"] = weight_scale * np.random.randn(50, 100)\n",
    "    network[\"b2\"] = np.zeros(100)\n",
    "\n",
    "    network[\"W3\"] = weight_scale * np.random.randn(100, 10)\n",
    "    network[\"b3\"] = np.zeros(10)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. 活性化関数 & 推論 (Functions)\n",
    "# ==========================================\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a, axis=-1, keepdims=True)  # バッチ対応\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a, axis=-1, keepdims=True)\n",
    "    return exp_a / sum_exp_a\n",
    "\n",
    "\n",
    "def predict(network, x):\n",
    "    \"\"\"\n",
    "    推論処理 (Forward Propagation)\n",
    "    x: 入力データ (Shape: [Batch_Size, 784])\n",
    "    \"\"\"\n",
    "    W1, W2, W3 = network[\"W1\"], network[\"W2\"], network[\"W3\"]\n",
    "    b1, b2, b3 = network[\"b1\"], network[\"b2\"], network[\"b3\"]\n",
    "\n",
    "    # Layer 1\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "\n",
    "    # Layer 2\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "\n",
    "    # Output Layer\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. 実行 & 可視化 (Main Execution)\n",
    "# ==========================================\n",
    "# データの取得\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "# --- データの確認 (Visual Check) ---\n",
    "# 最初の1枚を元の画像に戻して表示\n",
    "plt.figure(figsize=(3, 3))\n",
    "img = x[0].reshape(28, 28)  # 784 -> 28x28 に戻す\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label: {t[0]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\" >>> 3.6.2 Inference (Single Data)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1枚だけ推論してみる\n",
    "x_sample = x[0]  # Shape: (784,)\n",
    "# predictは (N, 784) を期待しているので、次元を追加する\n",
    "y_sample = predict(network, x_sample[np.newaxis, :])\n",
    "\n",
    "print(f\"Input Shape: {x_sample.shape}\")\n",
    "print(f\"Output Probabilities: \\n{y_sample[0]}\")\n",
    "pred_label = np.argmax(y_sample)\n",
    "print(f\"Predicted Label: {pred_label} (Correct: {t[0]})\")\n",
    "print(\"※ ランダム重みなので外れて当然である。\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\" >>> 3.6.3 Batch Processing (High Speed)\")\n",
    "print(\"=\" * 50)\n",
    "# バッチ処理: 100枚ずつ一気に計算する\n",
    "# 行列積の力で、1枚ずつループするより圧倒的に速い。\n",
    "\n",
    "batch_size = 100\n",
    "accuracy_cnt = 0\n",
    "\n",
    "print(f\"Total Images: {len(x)}\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i : i + batch_size]  # (100, 784)\n",
    "    y_batch = predict(network, x_batch)  # (100, 10)\n",
    "\n",
    "    # 最も確率が高いインデックスを取得\n",
    "    p = np.argmax(y_batch, axis=1)  # (100,)\n",
    "\n",
    "    # 正解数をカウント\n",
    "    accuracy_cnt += np.sum(p == t[i : i + batch_size])\n",
    "\n",
    "print(f\"\\nAccuracy: {float(accuracy_cnt) / len(x):.4f}\")\n",
    "print(\"※ 約0.1 (10%) になれば正常。10個の数字を適当に選んでいるだけだからだ。\")\n",
    "\n",
    "# （...先ほどのコードの続き...）\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"【⚡ Deus Ex Machina's Lecture: The Architecture of Thought】\")\n",
    "print(\"=\" * 60)\n",
    "print(\"まりよ、今まさに汝のPC内で「1万枚の画像」が瞬時に処理された。\")\n",
    "print(\"このスピードの秘密こそが「バッチ処理（Batch Processing）」と「行列演算」だ。\")\n",
    "print(\"\\n1. データの変形 (Flatten):\")\n",
    "print(\"   28x28の「絵」は、784次元の「数字の列」に解体された。\")\n",
    "print(\"   AIにとって、画像とは「空間」ではなく「信号の羅列」に過ぎない。\")\n",
    "print(\"\\n2. 情報の圧縮と抽象化:\")\n",
    "print(\"   784個の信号は、重みW1によって50個に圧縮され、W2で100個に拡張され、\")\n",
    "print(\"   最後にW3で「10個の確率（0〜9）」へと変換された。\")\n",
    "print(\"   [Image of Neural Network Architecture]\")\n",
    "print(\n",
    "    \"   この「層」をくぐる過程で、情報は『ピクセルの明暗』から『数字の特徴』へと昇華される。\"\n",
    ")\n",
    "print(\"\\n3. 現状の課題:\")\n",
    "print(\"   Accuracyは約0.1 (10%)。これは重みパラメータが「ランダム」だからだ。\")\n",
    "print(\"   回路（脳の構造）は完成している。だが、シナプス（重み）が繋がっていない。\")\n",
    "print(\"\\n次章『学習』にて、我々はこの重み W を、データから自動的に修正させる。\")\n",
    "print(\"その時、この回路はただの計算機から「知能」へと進化するだろう。\")\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696204a",
   "metadata": {},
   "source": [
    "# Chapter４\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafac476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2f1b549",
   "metadata": {},
   "source": [
    "# Chapter５\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740705ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74fb42bd",
   "metadata": {},
   "source": [
    "# Chapter６\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec8b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bbfad1a",
   "metadata": {},
   "source": [
    "# Chapter７\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e6eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fb8bd46",
   "metadata": {},
   "source": [
    "# Chapter８\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95db95f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
